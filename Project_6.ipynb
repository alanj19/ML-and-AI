{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "for dirname, _, filenames in os.walk(\"./\"):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Frame the problem\n",
    "Using the customer description, Define the problem your trying to solve in your own words (remember this is not technial but must be specific so the customer understands the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to train a computer to play the game Tetris on its own. Instead of programming the computer with fixed rules, I want it to learn through experience. The computer will observe the game state, try different moves, and learn which choices help it survive longer and clear more rows. Over time, the system should improve and become able to score higher without any human input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Get the Data \n",
    "Define how you recieved the data (provided, gathered..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the model starts learning it needs example gameplay results. The Tetris environment was already provided so I used the Random agent and Greedy agent to play several Tetris games where I recorded how many pieces were placed before losing and how many rows were cleared. This data will give us a baseline performance to compare against later on our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: pieces=17, rows=0\n",
      "Episode 1: pieces=17, rows=0\n",
      "Episode 2: pieces=24, rows=0\n",
      "Episode 3: pieces=20, rows=0\n",
      "Episode 4: pieces=15, rows=0\n",
      "Episode 5: pieces=20, rows=0\n",
      "Episode 6: pieces=19, rows=0\n",
      "Episode 7: pieces=25, rows=0\n",
      "Episode 8: pieces=15, rows=0\n",
      "Episode 9: pieces=17, rows=0\n",
      "Data saved to tetris_data.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from game import Game\n",
    "\n",
    "with open(\"tetris_data.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"episode\", \"pieces_dropped\", \"rows_cleared\"])\n",
    "\n",
    "    for episode in range(10):\n",
    "        g = Game(\"random\")\n",
    "        pieces, rows = g.run_no_visual()\n",
    "        writer.writerow([episode, pieces, rows])\n",
    "        print(f\"Episode {episode}: pieces={pieces}, rows={rows}\")\n",
    "\n",
    "print(\"Data saved to tetris_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Game 0 ---\n",
      "Result: 233, 78\n",
      "--- Running Game 1 ---\n",
      "Result: 12891, 5364\n",
      "--- Running Game 2 ---\n",
      "Result: 5383, 2235\n",
      "--- Running Game 3 ---\n",
      "Result: 684, 267\n",
      "--- Running Game 4 ---\n",
      "Result: 5143, 2129\n",
      "--- Running Game 5 ---\n",
      "Result: 2046, 842\n",
      "--- Running Game 6 ---\n",
      "Result: 946, 378\n",
      "--- Running Game 7 ---\n",
      "Result: 12238, 5093\n",
      "--- Running Game 8 ---\n",
      "Result: 25869, 10756\n",
      "--- Running Game 9 ---\n",
      "Result: 26259, 10915\n",
      "\n",
      "Data saved to tetris_data2.csv\n"
     ]
    }
   ],
   "source": [
    "import os, sys, csv\n",
    "\n",
    "os.chdir(\"/home/jupyter-1006929/my_tetris_project\")\n",
    "sys.path.append(os.path.join(os.getcwd(), \"src\"))\n",
    "\n",
    "from game import Game\n",
    "\n",
    "output_file = \"tetris_data2.csv\"\n",
    "\n",
    "with open(output_file, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"episode\", \"pieces_dropped\", \"rows_cleared\"])  \n",
    "\n",
    "    for episode in range(10):   \n",
    "        print(f\"--- Running Game {episode} ---\")\n",
    "        g = Game(\"greedy\")      \n",
    "        pieces, rows = g.run_fast()   \n",
    "        print(f\"Result: {pieces}, {rows}\")\n",
    "\n",
    "        writer.writerow([episode, pieces, rows])\n",
    "\n",
    "print(f\"\\nData saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Explore the Data\n",
    "Gain insights into the data you have from step 2, making sure to identify any bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After collecting gameplay results from the random Tetris agent, the dataset was analyzed to understand its performance. The data showed that the agent dropped between 14 and 24 pieces per game, with an average of around 20.3 pieces. However, the agent cleared zero rows in every game, indicating that it was not attempting to score points at all.\n",
    "\n",
    "Because row clearing never occurred, there was no variation in the rows_cleared column, and therefore no meaningful correlation could be computed between pieces dropped and rows cleared. This confirms that the random agentâ€™s behavior is purely survival-based and not strategic. The slight variation in the number of pieces dropped simply reflects randomness in board placement rather than intentional play. This will serve as a baseline for the creation of my actual model.\n",
    "\n",
    "The greedy agent dropped between 233 and 26,259 pieces per game and cleared between 78 and 10,915 rows, with averages of 9,169 pieces and 3,806 rows cleared. This shows that it is actually thinking and making strategic moves as it should. There is almost a perfect correlation between pieces placed and rows cleared, so this confirms this will work as a good baseline for our actual model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   episode  pieces_dropped  rows_cleared\n",
      "0        0              19             0\n",
      "1        1              19             0\n",
      "2        2              23             0\n",
      "3        3              21             0\n",
      "4        4              21             0\n",
      "5        5              19             0\n",
      "6        6              24             0\n",
      "7        7              14             0\n",
      "8        8              23             0\n",
      "9        9              20             0\n",
      "\n",
      "Summary Statistics:\n",
      "        episode  pieces_dropped  rows_cleared\n",
      "count  10.00000       10.000000          10.0\n",
      "mean    4.50000       20.300000           0.0\n",
      "std     3.02765        2.869379           0.0\n",
      "min     0.00000       14.000000           0.0\n",
      "25%     2.25000       19.000000           0.0\n",
      "50%     4.50000       20.500000           0.0\n",
      "75%     6.75000       22.500000           0.0\n",
      "max     9.00000       24.000000           0.0\n",
      "\n",
      "Correlation Matrix:\n",
      "                 episode  pieces_dropped  rows_cleared\n",
      "episode         1.000000       -0.006395           NaN\n",
      "pieces_dropped -0.006395        1.000000           NaN\n",
      "rows_cleared         NaN             NaN           NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"tetris_data.csv\")\n",
    "print(df)\n",
    "\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "print(df.corr())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Preview:\n",
      "   episode  pieces_dropped  rows_cleared\n",
      "0        0             233            78\n",
      "1        1           12891          5364\n",
      "2        2            5383          2235\n",
      "3        3             684           267\n",
      "4        4            5143          2129\n",
      "\n",
      "Summary Statistics:\n",
      "        episode  pieces_dropped  rows_cleared\n",
      "count  10.00000       10.000000     10.000000\n",
      "mean    4.50000     9169.200000   3805.700000\n",
      "std     3.02765     9972.043118   4152.154302\n",
      "min     0.00000      233.000000     78.000000\n",
      "25%     2.25000     1221.000000    494.000000\n",
      "50%     4.50000     5263.000000   2182.000000\n",
      "75%     6.75000    12727.750000   5296.250000\n",
      "max     9.00000    26259.000000  10915.000000\n",
      "\n",
      "Correlation Matrix:\n",
      "                 episode  pieces_dropped  rows_cleared\n",
      "episode         1.000000        0.656991      0.656758\n",
      "pieces_dropped  0.656991        1.000000      0.999999\n",
      "rows_cleared    0.656758        0.999999      1.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"tetris_data2.csv\")\n",
    "\n",
    "print(\"\\nData Preview:\")\n",
    "print(data.head())\n",
    "\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(data.describe())\n",
    "\n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "print(data.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Prepare the Data\n",
    "\n",
    "\n",
    "Apply any data transformations and explain what and why\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare the data I applied a normalization step because the two values we are using our measured on different scales are pieces dropped range from 17-26, while rows cleared is usually 0-1. To prevent this imbalance, I used Min-Max normalization,which rescales both features to a scale from 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"tetris_data.csv\")\n",
    "\n",
    "def min_max_normalize(col):\n",
    "    return (col - col.min()) / (col.max() - col.min())\n",
    "\n",
    "df[\"pieces_dropped_norm\"] = min_max_normalize(df[\"pieces_dropped\"])\n",
    "df[\"rows_cleared_norm\"] = min_max_normalize(df[\"rows_cleared\"])\n",
    "\n",
    "print(\"\\nNormalized Data:\")\n",
    "print(df[[\"episode\", \"pieces_dropped_norm\", \"rows_cleared_norm\"]])\n",
    "\n",
    "df.to_csv(\"tetris_data_normalized.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, I plan to build a Reinforcement Learning model to play Tetris. Unlike the greedy agent the RL model learns from experience by interacting with the game and receiving rewards for good moves. This approach is better suited for Tetris because strong play  often requires long-term planning, such as arranging pieces to clear multiple lines later instead of only focusing on immediate placement. I will probably use a DQN, which allows the agent to estimate which moves will lead to better results by analyzing the board state and improving those decisions over many played games."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model the data\n",
    "Using selected ML models, experment with your choices and describe your findings. Finish by selecting a Model to continue with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Fine Tune the Model\n",
    "\n",
    "With the select model descibe the steps taken to acheve the best rusults possiable \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Present\n",
    "In a customer faceing Document provide summery of finding and detail approach taken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Launch the Model System\n",
    "Define your production run code, This should be self susficent and require only your model pramaters \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
